{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train and Optimize a Support Vector Machine and a Naive Bayes Classifier using Scikit-learn\n",
    "\n",
    "<span style=\"color:#1565C0\">\n",
    " **Support Vector Machines and Naive Bayes classifier example with GridSearchCV hyperparamter optimization using sklearn-pipelines**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "This example should support the reader's familiarity with training and tuning two important classes of classifier models.  A grid search is implemented for tuning each model over all the possible combinations of hyperparameters passed to the `GridSearchCV` function.  Cross-validation is then used to measure the efficacy of each hyperparameter combination and choose the variant with the highest accuracy as the most optimized.  These tasks are coordinated by **scikit-learn**'s `make_pipeline` function so that the code is intuitive to review.\n",
    "\n",
    "### Data Description\n",
    "For the sake of fast-tracking through the data hygenie phases, the data used in this example is the iris flower dataset that is shipped with **scikit-learn**.  It's boring but for explanation purposes it is useful.\n",
    "\n",
    "### Classification Goal\n",
    "The goal with this data was to accurately classify the species of *iris* flower genus as either:\n",
    "\n",
    "* <span style=\"color:#1B5E20;\">setosa</span>\n",
    "* <span style=\"color:#1B5E20;\">versicolor</span>\n",
    "* <span style=\"color:#1B5E20;\">virginica</span>\n",
    "\n",
    "Classification is based on the following features of each flower instance:\n",
    "\n",
    "* <span style=\"color:#E65100\">sepal length</span>\n",
    "* <span style=\"color:#E65100\">sepal width</span>\n",
    "* <span style=\"color:#E65100\">petal length</span>\n",
    "* <span style=\"color:#E65100\">petal width</span>\n",
    "\n",
    "### Dependencies\n",
    "The following Python packages are requried for this script.\n",
    "\n",
    " - scikit-learn \n",
    " - pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "  <span style=\"color:SlateBlue\">\n",
    "    **Step 1**\n",
    "  </span>\n",
    "  : Load and Learn the Data\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for loading the data\n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data variable as 'flowers'\n",
    "flowers = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data matrix features and a sample of its instances\n",
    "# The instances of this matrix will serve as the independent variables\n",
    "df = pd.DataFrame(data=flowers.data, columns=flowers.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iris Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Iris Class\n",
       "count  150.000000\n",
       "mean     1.000000\n",
       "std      0.819232\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      2.000000\n",
       "max      2.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a description of the dependent variables\n",
    "df_target = pd.DataFrame(data=flowers.target, columns=['Iris Class'])\n",
    "df_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Corresponding iris class names mapped as nominal integer data: 0, 1, or 2\n",
    "print(flowers.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "  <span style=\"color:SlateBlue\">\n",
    "    **Step 2**\n",
    "  </span>\n",
    "  : Split into Train & Test Subsets\n",
    "</font>\n",
    "\n",
    "The beautiful developers of **scikit-learn** have implemented a function that makes this step a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = flowers.data\n",
    "y = flowers.target\n",
    "\n",
    "# Split the data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to sneak into this step the variable that describes the *k* for the K-Fold cross validation, where *k* is the number of folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> K-Fold Cross Validation:\n",
    "\n",
    "> The data set is divided into $k$ equal sized subsets, with one subset as the testing set and the $k-1$ remaining subsets serving as the training sets.  This process is then repeated $k$ times on the data.  For each of the $k$ folds, the training sets are used to predict the testing set and the average error is then calculated as:\n",
    "\n",
    "\n",
    "> $$ CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "> where $k$ is the count of subsets, $y_i$ is a vector of the actual values, and $\\hat{y_i}$ is a vector of the predicted values when trained on the other $k-1$ subsets, all for each repitition $i$ of the total $k$ folds.\n",
    "\n",
    "We will perform 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define 10 fold cross-validation\n",
    "cv = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "  <span style=\"color:SlateBlue\">\n",
    "    **Step 3**\n",
    "  </span>\n",
    "  : Build, Train, & Test the SVM Pipeline\n",
    "</font>\n",
    "\n",
    "We will use a `pipeline` to organize our grid of possible hyperparameters.  Pipelines will make your life easier and keep your code reader friendly.  They are a lot like Samuel Adams, in that they're always a good decision.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The format for the hyperparameter dictionary variables are always the same when using `make_pipeline`.  \n",
    "\n",
    "The format is **lowercase transformer/estimator object name** + 2 underscores + **hyperparamter** = list of desired hyperparameter values.\n",
    "\n",
    "For example, if we wanted to use boring old `sklearn.linear_model.LinearRegression` as our estimator we would define our paramter dictionary object as `dict(linearregression__fit_intercept=[True, False], linearregression__normalize=[True, False])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Define the pipeline object\n",
    "svc_pipeline = make_pipeline(SVC())\n",
    "\n",
    "# Define parameter dictionary that we will pass into GridSearchCV\n",
    "svc_parameters = dict(svc__kernel=['linear', 'poly', 'rbf'],\n",
    "                      svc__degree=[2, 3, 4],\n",
    "                      svc__shrinking=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `GridSearchCV` object and fit the training data to it.\n",
    "The scoring parameter here is defined as `f1_micro` which calculates the F1 accuracy \"metrics globally by counting the total true positives, false negatives, and false positives.\" [Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "<span style=\"color:red;font-weight:bold;\">Notice</span>:  We input the 10-fold cross-validation object `cv` into our `GridSearchCV` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'svc__kernel': ['linear', 'poly', 'rbf'], 'svc__degree': [2, 3, 4], 'svc__shrinking': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model & input the hyperparameter dictionary objects into the GridSearchCV function\n",
    "svm_model = GridSearchCV(svc_pipeline, param_grid=svc_parameters, scoring='f1_micro', cv=cv)\n",
    "\n",
    "# Test the SVC estimator with its grid of hyperparamters on the training set\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an alphabet soup is sign that everything worked.\n",
    "\n",
    "Let's find out what hyperparameters were found to score the greatest F1 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__degree': 2, 'svc__kernel': 'linear', 'svc__shrinking': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "We can see what hyperparameters were found to yield the highest accuracy above.\n",
    "\n",
    "The `svc_model` estimator object will use the best hyperparameters when we call `.predict(X_test)` to test the accuracy of our optimized model.  The output is made reader friendly by using the `sklearn.metrics.classification_report` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00         8\n",
      "          2       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict the values of y_test from inputting X_test\n",
    "svm_y_predicted = svm_model.predict(X_test)\n",
    "\n",
    "# Print a reader friendly classification report\n",
    "svm_report = classification_report(y_test, svm_y_predicted)\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't expect results like this when working with real world data.  A perfect accuracy score on a real life data set is a sign that something is wrong.  Possible sources of error could be overfitting or insufficient testing sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "<font size=\"4\">\n",
    "  <span style=\"color:SlateBlue\">\n",
    "    **Step 4**\n",
    "  </span>\n",
    "  : Build, Train, and Test Naive Bayes Estimator\n",
    "</font>\n",
    "\n",
    "The procedure here is nearly identical to that of step 3 except now we will use the `GaussianNB` estimator.  We will forgo hyperparamter optimization here, but still use `GridSearchCV` for it's easy cross-validation ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None, steps=[('gaussiannb', GaussianNB(priors=None))]),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define pipeline object\n",
    "nb_pipeline = make_pipeline(GaussianNB())\n",
    "\n",
    "# Define the model & input an empty hyperparameter dictionary\n",
    "nb_model = GridSearchCV(nb_pipeline, param_grid=dict(), scoring='f1_micro', cv=cv)\n",
    "\n",
    "# Fit the training data\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.89      1.00      0.94         8\n",
      "          2       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the values using the Naive Bayes classifier\n",
    "nb_y_predicted = nb_model.predict(X_test)\n",
    "\n",
    "# Print a reader friendly classification report\n",
    "nb_report = classification_report(y_test, nb_y_predicted)\n",
    "print(nb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized SVM classifier was shown to be have an F1 accuracy score 3% greater than that of the Naive Bayes classifier.  Therefore when predicting the species of each *iris* genus flower instance based on sepal width/length and petal width/length, the SVM classifier should be preferred over the Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The point of this script is to give examples of how the different machine learning modules from the sklearn package can be used together to write code that is easy to read and hopefully intuitive to write.\n",
    "\n",
    "The code from steps 3 and 4 showcase how `GridSearchCV` can readily be utilized by an sklearn `pipeline` to choose the best hyperparameters for the specified estimator algorithm.  The best hyperparameters are chosen via the cross-validation object, which was defined as `cv`, using the F1 efficiency algorithm.\n",
    "\n",
    "I understand that the code may seem dense if you are at the initial stage of the learning curve with sklearn.  I did make a conscious effort to comment my code above in hopes that it would support reader understanding.  I encourage you to grab a different data set and emulate these pipelines until you feel comfortable with this process.\n",
    "\n",
    "If you believe, just from reading this script, that you can code better than me then I encourage you to write all your *better* code, by hand, on the back of the fast food wrapper that you had for lunch, and then immediately put it into the trash.  Everyone will be very impressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearning",
   "language": "python",
   "name": "mlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
